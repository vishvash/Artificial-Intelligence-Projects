# -*- coding: utf-8 -*-
"""BERT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IRgIKzTwCGewU8C6dkunfak8heX6ZKxr
"""

pip install transformers

"""Generate BERT embeddings"""

from transformers import BertModel, BertTokenizer

pip install torch

import torch

model = BertModel.from_pretrained('bert-base-uncased')

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

sentence = 'I love Paris'

tokens = tokenizer.tokenize(sentence)

print(tokens)

tokens = ['[CLS]'] + tokens + ['[SEP]']

print(tokens)

tokens = tokens + ['[PAD]'] + ['[PAD]']

print(tokens)

attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]

print(attention_mask)

token_ids = tokenizer.convert_tokens_to_ids(tokens)

print(token_ids)

token_ids = torch.tensor(token_ids).unsqueeze(0)

attention_mask = torch.tensor(attention_mask).unsqueeze(0)

hidden_rep, cls_head = model(token_ids, attention_mask = attention_mask)

print(hidden_rep.shape)

from transformers import BertModel, BertTokenizer
import torch

model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states = True)
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

sentence = 'I love Paris'
tokens = tokenizer.tokenize(sentence)
tokens = ['[CLS]'] + tokens + ['[SEP]']

tokens = tokens + ['[PAD]'] + ['[PAD]']
attention_mask = [1 if i!= '[PAD]' else 0 for i in tokens]

token_ids = tokenizer.convert_tokens_to_ids(tokens)

token_ids = torch.tensor(token_ids).unsqueeze(0)
attention_mask = torch.tensor(attention_mask).unsqueeze(0)

last_hidden_state, pooler_output, hidden_states = model(token_ids, attention_mask = attention_mask)

last_hidden_state.shape

torch.Size([1, 7, 768])

pooler_output.shape

len(hidden_states)

hidden_states[0].shape

torch.Size([1, 7, 768])

hidden_states[1].shape

torch.Size([1, 7, 768])

"""FINE-TUNING BERT for Sentiment Analysis"""

pip install nlp

pip install transformers

from transformers import BertForSequenceClassification, BertTokenizerFast, Trainer, TrainingArguments

from nlp import load_dataset

import torch

import numpy as np

!gdown https://drive.google.com/uc?id=11_M4ootuT7I1G0RlihcC0cA3Elqotlc-
dataset = load_dataset('csv', data_files='./imdbs.csv', split='train')

type(dataset)

dataset = dataset.train_test_split(test_size=0.3)

dataset

train_set = dataset['train']
test_set = dataset['test']

model = BertForSequenceClassification.from_pretrained('bert-base-uncased')

tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')

tokens = [ [CLS], I, love, Paris, [SEP] ]

input_ids = [101, 1045, 2293, 3000, 102]

token_type_ids = [0, 0, 0, 0, 0]

attention_mask = [1, 1, 1, 1, 1]

tokenizer('I love Paris')

tokenizer(['I love Paris', 'birds fly','snow fall'], padding = True, max_length=5)

def preprocess(data):
  return tokenizer(data['text'], padding=True, truncation=True)

train_set = train_set.map(preprocess, batched=True, batch_size=len(train_set))
test_set = test_set.map(preprocess, batched=True, batch_size=len(test_set))

train_set.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])
test_set.set_format('torch', columns=['input_ids', 'attention_mask', 'label'])

batch_size = 8
epochs = 2

warmup_steps = 500
weight_decay = 0.01

from google.colab import drive
drive.mount('/content/gdrive')

training_args = TrainingArguments(
output_dir='./results',
num_train_epochs=epochs,
per_device_train_batch_size=batch_size,
per_device_eval_batch_size=batch_size,
warmup_steps=warmup_steps,
weight_decay=weight_decay, 
evaluation_strategy='epoch',
logging_dir='/content/gdrive/MyDrive/logs',
)

trainer = Trainer(model=model,args=training_args,train_dataset=train_set,eval_dataset=test_set)

trainer.train()

trainer.evaluate()

"""Natural Language Inference"""

tokens = [ [CLS], He, is, playing, [SEP], He, is, sleeping [SEP]]

"""Question-Answering"""

from transformers import BertForQuestionAnswering, BertTokenizer

model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

tokenizer = BertTokenizer.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')

question = "What is the immune system?"
paragraph = "The immune system is a system of many biological structures and processes within an organism that protects against disease. To function properly, an immune system must detect a wide variety of agents, known as pathogens, from viruses to parasitic worms, and distinguish them from the organism's own healthy tissue."

question = '[CLS] ' + question + '[SEP]'
paragraph = paragraph + '[SEP]'

question_tokens = tokenizer.tokenize(question)
paragraph_tokens = tokenizer.tokenize(paragraph)

tokens = question_tokens + paragraph_tokens 
input_ids = tokenizer.convert_tokens_to_ids(tokens)

segment_ids = [0] * len(question_tokens)
segment_ids = [1] * len(paragraph_tokens)

input_ids = torch.tensor([input_ids])
segment_ids = torch.tensor([segment_ids])

start_scores, end_scores = model(input_ids, token_type_ids = segment_ids)

start_index = torch.argmax(start_scores)
end_index = torch.argmax(end_scores)

print(' '.join(tokens[start_index:end_index+1]))

a system of many biological structures and processes within an organism that protects against disease

"""ALBERT"""

!pip install transformers

from transformers import AlbertTokenizer, AlbertModel
pip install sentencepiece
pip install --upgrade torch


model = AlbertModel.from_pretrained('albert-base-v2')
tokenizer = AlbertTokenizer.from_pretrained('albert-base-v2')

sentence = "Paris is a beautiful city" 
inputs = tokenizer(sentence, return_tensors="pt")

print(inputs)

hidden_rep, cls_head = model(**inputs)

"""RoBERTa"""

tokens = [ [CLS], we, arrived, at, the, airport, in, time, [SEP] ]

tokens = [ [CLS], we, [MASK], at, the airport, in, [MASK], [SEP] ]

pip install transformers

import transformers
from transformers import RobertaConfig, RobertaModel, RobertaTokenizer

model = RobertaModel.from_pretrained('roberta-base')

model.config

tokenizer = RobertaTokenizer.from_pretrained("roberta-base")

tokenizer.tokenize('It was a great day')

tokenizer.tokenize('I had a sudden epiphany')

"""ELECTRA"""

from transformers import ElectraTokenizer, ElectraModel

model = ElectraModel.from_pretrained('google/electra-small-discriminator')

"""SpanBERT"""

import transformers
from transformers import pipeline

from transformers import pipeline

qa_pipeline = pipeline(
"question-answering",
model="mrm8488/spanbert-large-finetuned-squadv2",
tokenizer="SpanBERT/spanbert-large-cased"
)

results = qa_pipeline({
'question': "What is machine learning?",
'context': "Machine learning is a subset of artificial intelligence. It is widely for creating a variety of applications such as email filtering and computer vision"
})

print(results['answer'])

"""BERTSUM"""

!pip install torch==1.1.0 pytorch_transformers tensorboardX multiprocess pyrouge
!pip install googleDriveFileDownloader

pip install pytorch-pretrained-bert

cd /content/

!git clone https://github.com/nlpyang/BertSum.git

cd /content/BertSum/bert_data/

from googleDriveFileDownloader import googleDriveFileDownloader
gdrive = googleDriveFileDownloader()
gdrive.downloadFile("https://drive.google.com/uc?id=1x0d61LP9UAN389YN00z0Pv-7jQgirVg6&export=download")

!unzip /content/BertSum/bert_data/bertsum_data.zip

cd /content/BertSum/src

!python train.py -mode train -encoder classifier -dropout 0.1 -bert_data_path ../bert_data/cnndm -model_path ../models/bert_classifier -lr 2e-3 -visible_gpus 0 -gpu_ranks 0 -world_size 1 -report_every 50 -save_checkpoint_steps 1000 -batch_size 3000 -decay_method noam -train_steps 50 -accum_count 2 -log_file ../logs/bert_classifier -use_interval true -warmup_steps 10000

"""M-BERT, XLM, XLM-R"""

from transformers import BertTokenizer, BertModel

model = BertModel.from_pretrained('bert-base-multilingual-cased')

tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')

sentence = "C'est une si belle journée"

inputs = tokenizer(sentence, return_tensors="pt")

hidden_rep, cls_head = model(**inputs)

"""FlauBERT - French"""

from transformers import FlaubertTokenizer, FlaubertModel
import torch

model = FlaubertModel.from_pretrained('flaubert/flaubert_base_cased')

tokenizer = FlaubertTokenizer.from_pretrained('flaubert/flaubert_base_cased')

sentence = "Paris est ma ville préférée"

token_ids = tokenizer.encode(sentence)

token_ids = torch.tensor(token_ids).unsqueeze(0)

representation = model(token_ids)[0]

representation.shape

cls_rep = representation[:, 0, :]

"""BETO - Spanish"""

tokenizer = BertTokenizer.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')
model = BertModel.from_pretrained('dccuchile/bert-base-spanish-wwm-uncased')

from transformers import pipeline

predict_mask = pipeline(
"fill-mask",
model= "dccuchile/bert-base-spanish-wwm-uncased",
tokenizer="dccuchile/bert-base-spanish-wwm-uncased"
)

result = predict_mask('[MASK] los caminos llevan a Roma')

print(result)

"""BERTje - Dutch"""

from transformers import BertForNextSentencePrediction, BertTokenizer
from torch.nn.functional import softmax

model = BertForNextSentencePrediction.from_pretrained("wietsedv/bert-base-dutch-cased")

tokenizer = BertTokenizer.from_pretrained("wietsedv/bert-base-dutch-cased")

sentence_A = 'Ik woon in Amsterdam'
sentence_B = 'Een geweldige plek'

embeddings = tokenizer(sentence_A, sentence_B, return_tensors='pt')

logits = model(**embeddings)[0]

probs = softmax(logits, dim=1)

print(probs)

"""German BERT"""

from transformers import AutoTokenizer, AutoModel

model = AutoModel.from_pretrained("bert-base-german-cased")

tokenizer = AutoTokenizer.from_pretrained("bert-base-german-cased")

"""Chinese BERT"""

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("bert-base-chinese")
model = AutoModel.from_pretrained("bert-base-chinese")

tokenizer = BertTokenizer.from_pretrained("hfl/chinese-bert-wwm")
model = BertModel.from_pretrained("hfl/chinese-bert-wwm")

"""Japanese BERT"""

from transformers import AutoTokenizer
AutoModelmodel = AutoModel.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")
tokenizer = AutoTokenizer.from_pretrained("cl-tohoku/bert-base-japanese-whole-word-masking")

"""FinBERT - Finnish"""

tokenizer = BertTokenizer.from_pretrained("TurkuNLP/bert-base-finnish-uncased-v1")
model = BertModel.from_pretrained("TurkuNLP/bert-base-finnish-uncased-v1")

"""UmBERTo - Italian"""

tokenizer = AutoTokenizer.from_pretrained("Musixmatch/umberto-commoncrawl-cased-v1")
model = AutoModel.from_pretrained("Musixmatch/umberto-commoncrawl-cased-v1")

"""BERTimbau - Portuguese"""

from transformers import AutoModel
AutoTokenizertokenizer = AutoTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased') 
model = AutoModel.from_pretrained('neuralmind/bert-base-portuguese-cased')

"""RuBERT - Russian"""

from transformers import AutoTokenizer, AutoModel
tokenizer = AutoTokenizer.from_pretrained("DeepPavlov/rubert-base-cased")
model = AutoModel.from_pretrained("DeepPavlov/rubert-base-cased")

pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
model = SentenceTransformer('bert-base-nli-mean-tokens')

sentence = 'paris is a beautiful city'

sentence_representation = model.encode(sentence)

print(sentence_representation.shape)

import scipy
from sentence_transformers import SentenceTransformer, util

model = SentenceTransformer('bert-base-nli-mean-tokens')
sentence1 = 'It was a great day'
sentence2 = 'Today was awesome'

sentence1_representation = model.encode(sentence1)
sentence2_representation = model.encode(sentence2)

cosine_sim = util.pytorch_cos_sim(sentence1_representation,sentence2_representation)

print(cosine_sim)

from sentence_transformers import models,SentenceTransformer

word_embedding_model = models.Transformer('albert-base-v2')

pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), pooling_mode_mean_tokens=True, pooling_mode_cls_token=False, pooling_mode_max_tokens=False)

model = SentenceTransformer(modules=[word_embedding_model, pooling_model])

model.encode('Transformers are awesome')

from sentence_transformers import SentenceTransformer, util
import numpy as np

model = SentenceTransformer('bert-base-nli-mean-tokens')

master_dict = [
'How to cancel my order?',
'Please let me know about the cancellation policy?',
'Do you provide refund?',
'what is the estimated delivery date of the product?',
'why my order is missing?',
'how do i report the delivery of the incorrect items?'
]

inp_question = 'When is my product getting delivered?'

inp_question_representation = model.encode(inp_question, convert_to_tensor=True)

master_dict_representation = model.encode(master_dict, convert_to_tensor=True)

similarity = util.pytorch_cos_sim(inp_question_representation, master_dict_representation )

print('The most similar question in the master dictionary to given input question is:',master_dict[np.argmax(similarity)])

from sentence_transformers import SentenceTransformer, util
import scipy

model = SentenceTransformer('distiluse-base-multilingual-cased')

eng_sentence = 'thank you very much'
fr_sentence = 'merci beaucoup'

eng_sentence_embedding = model.encode(eng_sentence)
fr_sentence_embedding = model.encode(fr_sentence)

similarity = util.pytorch_cos_sim(eng_sentence_embedding,fr_sentence_embedding)

print('The similarity score is:',similarity)

from transformers import BartTokenizer, BartForConditionalGeneration

model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')
tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')

text = """Machine learning (ML) is the study of computer algorithms that improve automatically through experience.It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as training data, in order to make predictions or decisions without being explicitly programmed to do so.Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks."""

inputs = tokenizer([text], max_length=1024, return_tensors='pt')

summary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=True)

summary = ([tokenizer.decode(i, skip_special_tokens=True, clean_up_tokenization_spaces=False) for i in summary_ids])

print(summary)

pip install ktrain

import ktrain
from ktrain import text

import pandas as pd

!gdown https://drive.google.com/uc?id=1-8urBLVtFuuvAVHi0s000e7r0KPUgt9f
df = pd.read_json(r'reviews_Digital_Music_5.json',lines=True)

df.head()

df = df[['reviewText','overall']]
df.head()

sentiment = {1:'negative',2:'negative',3:'negative',4:'positive',5:'positive'}

df['sentiment'] = df['overall'].map(sentiment)

df = df[['reviewText','sentiment']]
df.head()

(x_train, y_train), (x_test, y_test), preproc = text.texts_from_df(train_df = df, text_column = 'reviewText', label_columns=['sentiment'], maxlen=100, max_features=100000, preprocess_mode='bert', val_pct=0.1)

text.print_text_classifiers()

model = text.text_classifier(name='bert', train_data = (x_train,y_train), preproc=preproc, metrics=['accuracy'])

learner = ktrain.get_learner(model = model, train_data=(x_train, y_train), val_data=(x_test, y_test), batch_size=32, use_multiprocessing = True)

learner.fit_onecycle(lr=2e-5, epochs=1,checkpoint_folder='output')

predictor = ktrain.get_predictor(learner.model, preproc)

predictor.predict('I loved the song')

from ktrain import text
import os
import shutil

!wget http://mlg.ucd.ie/files/datasets/bbc-fulltext.zip

!unzip bbc-fulltext.zip

from ktrain import text
import os

os.chdir(os.getcwd() + '/bbc')

text.SimpleQA.initialize_index('index')

text.SimpleQA.index_from_folder(folder_path='entertainment', index_dir='index')

qa = text.SimpleQA('index')

answers = qa.ask('who had a global hit with where is the love?')

qa.display_answers(answers[:5])

answers = qa.ask('who win at mtv europe awards?')
qa.display_answers(answers[:5])

!pip install wikipedia

import wikipedia

wiki = wikipedia.page('Pablo Picasso')

doc = wiki.content

print(doc[:1000])

from ktrain import text
ts = text.TransformerSummarizer()

ts.summarize(doc)

!pip install bert-serving-client
!pip install -U bert-serving-server[http]

!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
!unzip uncased_L-12_H-768_A-12.zip

from bert_serving.client import BertClient

bc = BertClient()

sentence1 = 'the weather is great today'
sentence2 = 'it looks like today the weather is pretty nice'

sent_rep1 = bc.encode([sentence1])
sent_rep2 = bc.encode([sentence2])

print(sent_rep1.shape, sent_rep2.shape)

from sklearn.metrics.pairwise import cosine_similarity
cosine_similarity(sent_rep1,sent_rep2)

!nohup bert-serving-start -pooling_strategy NONE -max_seq_len=20 \
-model_dir=./uncased_L-12_H-768_A-12 > out.file 2>&1 &

from bert_serving.client import BertClient

sentence = 'The weather is great today'

vec = bc.encode([sentence])

print(vec.shape)